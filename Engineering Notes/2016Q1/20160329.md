# ALBA
* 300 msec is what we see, network + rocksdb -- 100 usecs, where do we lose 200 msec
    * Rocksdb latency? rocksdb latency (when in pagecache) = avg 91us
    * Change block cache of rocksdb for optimal
    * https://www.thomas-krenn.com/en/wiki/Linux_Multi-Queue_Block_IO_Queueing_Mechanism_(blk-mq) blk-mq (Multi-Queue Block IO Queueing Mechanism) is a new framework for the Linux block layer that was introduced with Linux Kernel 3.13, and which has become feature-complete with Kernel 3.16.[1] Blk-mq allows for over 15 million IOPS with high-performance flash devices (e.g. PCIe SSDs) on 8-socket servers, though even single and dual socket servers also benefit considerably from blk-mq.[2] To use a device with blk-mq, the device must support the respective driver.
* RDMA code almost finished (end of the weekish)
* 1 process per ASD
    * Test multiple clients per ASDs
    * Test multiple ASDs per SSD (single client, multiple clients)
    * Test multiqueue
* Timing for Network request
    * Lookup in rocksdb next
    * open >> fadvise >> lseek >> sendfile >> fadvise >> close
* Odirect? Might be a good strategy, if the page cache or system calls are the issue. Maybe only for read process.
* Letâ€™s do a test against an intel pcie-flash or memblaze (how fast can it read!)
* How many file descriptors can we keep open? 100K is possible. Maybe LRU to cache file descriptors?
